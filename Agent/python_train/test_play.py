"""
ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ ìŠ¤í¬ë¦½íŠ¸
buckshot_evalê³¼ buckshot_next ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ ê²Œì„ì„ í”Œë ˆì´í•©ë‹ˆë‹¤.
"""
"""
ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ ìŠ¤í¬ë¦½íŠ¸ (ì‹œì  ë³€í™˜ ìˆ˜ì • ë²„ì „)
"""
import numpy as np
import os
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ model, utils, train_refë¥¼ importí•  ìˆ˜ ìˆë„ë¡ í•¨
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import Agent
from game_env import GameEnvironment, Player, ActionType
from train_ref import get_opponent_action

def test_play(
    num_games: int = 10,
    checkpoint_dir = "Agents",
    max_hp: int = 4,
    verbose: bool = True
):
    if checkpoint_dir is None:
        checkpoint_dir = "Agents"
    
    print("=" * 70)
    print("Buckshot Roulette Test Play (Invariance Applied)")
    print("=" * 70)
    
    # ë‘ ì—ì´ì „íŠ¸ ìƒì„± (epsilon=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ íƒí—˜ ì—†ì´ í”Œë ˆì´)
    agent_red = Agent(gamma=0.99, epsilon=0.0, lr=4e-4, input_dims=[20], n_actions=7,
                      mem_size=1000000, batch_size=64, eps_min=0.0, eps_dec=0.0,
                      replace=100, checkpoint_dir=checkpoint_dir)
    
    agent_blue = Agent(gamma=0.99, epsilon=0.0, lr=4e-4, input_dims=[20], n_actions=7,
                       mem_size=1000000, batch_size=64, eps_min=0.0, eps_dec=0.0,
                       replace=100, checkpoint_dir=checkpoint_dir)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (buckshot_eval ì‚¬ìš©)
    try:
        agent_red.load_models()
        agent_blue.load_models()
        print("âœ“ Checkpoints loaded successfully")
    except Exception as e:
        print(f"âœ— Error loading checkpoints: {e}")
        return
    
    env = GameEnvironment(max_hp=max_hp)
    red_wins, blue_wins = 0, 0
    red_scores, blue_scores, game_lengths = [], [], []
    action_names = [action.name for action in ActionType]
    
    for game_num in range(num_games):
        obs = env.reset() #
        done = False
        red_score, blue_score, step_count = 0.0, 0.0, 0
        
        while not done and step_count < 1000:
            step_count += 1
            current_player = Player.RED if env.current_turn == Player.RED else Player.BLUE
            agent = agent_red if current_player == Player.RED else agent_blue
            
            # [í•µì‹¬ ìˆ˜ì •] ì—ì´ì „íŠ¸ì—ê²Œ ì£¼ê¸° ì „ ì‹œì ì„ í•­ìƒ 'ë³¸ì¸ ì¤‘ì‹¬'ìœ¼ë¡œ ë³€í™˜
            # Red(1P)ì¼ ë•ŒëŠ” ë°ì´í„°ë¥¼ Swapí•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë³¸ì¸ì„ Blue(0P)ë¼ê³  ì°©ê°í•˜ê²Œ ë§Œë“¦
            state_for_agent = env.preprocess_state(obs) 
            
            # ë³€í™˜ëœ ìƒíƒœë¡œ ì•¡ì…˜ ê²°ì •
            action, _ = agent.choose_action(state_for_agent)
            
            # ì‹¤ì œ í™˜ê²½ì— ì•¡ì…˜ ì ìš© (ì›ë³¸ ê´€ì¸¡ê°’ ê¸°ì¤€)
            next_obs, reward, done, info = env.step(action)
            
            if current_player == Player.RED: red_score += reward
            else: blue_score += reward
            
            if verbose and step_count <= 10: # ë¡œê·¸ ì¶œë ¥
                p_name = "Red" if current_player == Player.RED else "Blue"
                print(f"Step {step_count} | {p_name} | Action: {action_names[action]} | Reward: {reward:.1f}")
            
            obs = next_obs
            
            if done:
                if env.red_lives <= 0: blue_wins += 1
                elif env.blue_lives <= 0: red_wins += 1
                break
        
        red_scores.append(red_score)
        blue_scores.append(blue_score)
        game_lengths.append(step_count)
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print(f"Final Results over {num_games} games")
    print(f"Red Win Rate: {red_wins/num_games*100:.1f}% | Avg Score: {np.mean(red_scores):.2f}")
    print(f"Blue Win Rate: {blue_wins/num_games*100:.1f}% | Avg Score: {np.mean(blue_scores):.2f}")
    print(f"Average Steps: {np.mean(game_lengths):.1f}")
    print("=" * 70)

if __name__ == "__main__":
    test_play(num_games=100)

def test_against_teacher(
    num_games: int = 100,
    checkpoint_dir = "Agents",
    max_hp: int = 4,
    teacher_level: int = 4  # ì„ ìƒë‹˜ ë‚œì´ë„ ì„¤ì •
):
    if checkpoint_dir is None:
        checkpoint_dir = "Agents"
    
    # ë‚´ ì—ì´ì „íŠ¸ ìƒì„± (Blue ì§„ì˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    my_agent = Agent(gamma=0.99, epsilon=0.0, lr=0, input_dims=[20], n_actions=7,
                     mem_size=1, batch_size=1, checkpoint_dir=checkpoint_dir)
    
    try:
        my_agent.load_models()
        print(f"âœ… ë‚´ ì—ì´ì „íŠ¸ ë¡œë“œ ì™„ë£Œ (Location: {checkpoint_dir})")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    env = GameEnvironment(max_hp=max_hp)
    my_wins, teacher_wins = 0, 0
    my_scores, teacher_scores = [], []

    print("=" * 70)
    print(f"ğŸ¥Š ëŒ€ê²° ì‹œì‘: ë‚´ ì—ì´ì „íŠ¸ (Blue) vs Level {teacher_level} ì„ ìƒë‹˜ (Red)")
    print("=" * 70)

    for game_num in range(num_games):
        obs = env.reset()
        done = False
        my_score, teacher_score = 0.0, 0.0
        
        while not done:
            if env.current_turn == Player.BLUE:
                # 1. ë‚´ ì—ì´ì „íŠ¸ í„´ (ì‹œì  ë³€í™˜ í›„ ì•¡ì…˜ ê²°ì •)
                state_for_agent = env.preprocess_state(obs)
                action, _ = my_agent.choose_action(state_for_agent)
                obs, reward, done, _ = env.step(action)
                my_score += reward
            else:
                # 2. Level 4 ì„ ìƒë‹˜ í„´ (ê¸°ì¡´ Rule-based ë¡œì§ ì‚¬ìš©)
                action = get_opponent_action(obs, teacher_level)
                obs, reward, done, _ = env.step(action)
                teacher_score += reward
            
        # ê²°ê³¼ ì§‘ê³„
        if env.blue_lives > 0 and env.red_lives <= 0:
            my_wins += 1
        elif env.red_lives > 0 and env.blue_lives <= 0:
            teacher_wins += 1
            
        my_scores.append(my_score)
        teacher_scores.append(teacher_score)

    # ìµœì¢… ìŠ¹ë¥  ë³´ê³ 
    print("\n" + "=" * 70)
    print(f"ğŸ“Š ìµœì¢… ì„±ì  ({num_games} ê²Œì„)")
    print(f"ğŸ† ë‚´ ì—ì´ì „íŠ¸ ìŠ¹ë¥ : {my_wins/num_games*100:.1f}% (Avg Score: {np.mean(my_scores):.2f})")
    print(f"ğŸ‘¨â€ğŸ« ì„ ìƒë‹˜ AI ìŠ¹ë¥ : {teacher_wins/num_games*100:.1f}% (Avg Score: {np.mean(teacher_scores):.2f})")
    print("=" * 70)

if __name__ == "__main__":
    test_against_teacher(num_games=100)