import numpy as np
import os
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import Agent
from utils import plotLearning
from game_env import GameEnvironment, Player

def train_pure_self_play(
    num_games: int = 10000,
    checkpoint_interval: int = 300,
    save_dir: str = 'Agents',
    load_checkpoint: bool = True,
    max_hp: int = 4
):
    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(save_dir): os.makedirs(save_dir)

    # epsilon: ê²Œì„ ìˆ˜ ê¸°ì¤€ ì„ í˜• ê°ì†Œ (num_games ë™ì•ˆ 0.1 â†’ 0.01)
    eps_initial, eps_min = 0.1, 0.01
    eps_dec_per_game = (eps_initial - eps_min) / num_games

    # ë©”ì¸ í•™ìŠµ ì—ì´ì „íŠ¸ í•˜ë‚˜ë§Œ ì‚¬ìš©
    main_agent = Agent(gamma=0.99, epsilon=eps_initial, lr=5e-5,
                       input_dims=[20], n_actions=7, mem_size=100000,
                       batch_size=64, eps_min=eps_min, eps_dec=eps_dec_per_game, replace=100,
                       checkpoint_dir=save_dir)

    if load_checkpoint:
        try:
            main_agent.load_models()
            print(">>> ê¸°ì¡´ buckshot_eval ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except:
            print(">>> ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")

    env = GameEnvironment(max_hp=max_hp)
    
    scores_history = []
    eps_history = []

    print(f"ğŸš€ ìˆœìˆ˜ Self-Play í•™ìŠµ ì‹œì‘ (ì´ {num_games} ê²Œì„)")

    for game_num in range(1, num_games + 1):
        obs = env.reset() #
        done = False
        score = 0
        
        while not done:
            # í˜„ì¬ í„´ì¸ í”Œë ˆì´ì–´ì˜ ê´€ì ìœ¼ë¡œ ì‹œì  ë³€í™˜
            view = env.preprocess_state(obs)
            mask = env.get_action_mask()
            # ë©”ì¸ ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ì§€ëŠ¥ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ (BLUE/RED ê³µí†µ, action masking ì ìš©)
            action, _ = main_agent.choose_action(view, action_mask=mask)
            
            if env.current_turn == Player.BLUE:
                # BLUE í„´
                next_obs, reward, done, _ = env.step(action)
                next_view = env.preprocess_state(next_obs)
                main_agent.store_transition(view, action, reward, next_view, int(done))
                main_agent.learn()
                score += reward
                obs = next_obs
            else:
                # RED(1ë²ˆ) í„´: RED ê²½í—˜ë„ BLUE ê´€ì ìœ¼ë¡œ ì €ì¥ í›„ í•™ìŠµ
                # REDê°€ ì–»ì€ ë³´ìƒ = BLUE ì…ì¥ì—ì„  ì†í•´ì´ë¯€ë¡œ -reward ë¡œ ì €ì¥
                next_obs, reward, done, _ = env.step(action)
                next_view = env.preprocess_state(next_obs)
                main_agent.store_transition(view, action, reward, next_view, int(done))
                main_agent.learn()
                obs = next_obs

        scores_history.append(score)
        eps_history.append(main_agent.epsilon)
        main_agent.decrease_epsilon()  # ê²Œì„ 1íšŒë§ˆë‹¤ epsilon 1íšŒ ê°ì†Œ

        # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥ (100ê²Œì„ë§ˆë‹¤)
        if game_num % 100 == 0:
            main_agent.save_models()
            
        if game_num % checkpoint_interval == 0:
            avg_score = np.mean(scores_history[-checkpoint_interval:])
            print(f"Ep {game_num} | Avg Score: {avg_score:.1f} | Eps: {main_agent.epsilon:.4f}")

    # --- í•™ìŠµ ì¢…ë£Œ í›„ ê·¸ë˜í”„ ìƒì„± ---
    print(">>> í•™ìŠµ ì¢…ë£Œ. ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    x = [i+1 for i in range(len(scores_history))]
    graph_filename = 'pure_self_play_results.png'
    plotLearning(x, scores_history, eps_history, graph_filename)
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {graph_filename}")

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--games", type=int, default=10000, help="í•™ìŠµí•  ê²Œì„ ìˆ˜ (ì§§ê²Œ: 200~500)")
    p.add_argument("--no-load", action="store_true", help="ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„°")
    p.add_argument("--save-dir", type=str, default="Agents", help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ")
    args = p.parse_args()
    train_pure_self_play(
        num_games=args.games,
        load_checkpoint=not args.no_load,
        save_dir=args.save_dir,
    )