import numpy as np
import os
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import Agent
from utils import plotLearning
from game_env import GameEnvironment, Player

def _best_score_path(save_dir: str) -> str:
    return os.path.join(save_dir, "best_avg_score.txt")

def train_pure_self_play(
    num_games: int = 10000,
    checkpoint_interval: int = 300,
    save_dir: str = 'Agents',
    load_checkpoint: bool = True,
    max_hp: int = 4
):
    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(save_dir): os.makedirs(save_dir)

    # epsilon: ê²Œì„ ìˆ˜ ê¸°ì¤€ ì„ í˜• ê°ì†Œ (num_games ë™ì•ˆ 0.1 â†’ 0.01)
    eps_initial, eps_min = 0.1, 0.01
    eps_dec_per_game = (eps_initial - eps_min) / num_games

    # ë©”ì¸ í•™ìŠµ ì—ì´ì „íŠ¸ í•˜ë‚˜ë§Œ ì‚¬ìš©
    main_agent = Agent(gamma=0.99, epsilon=eps_initial, lr=5e-5,
                       input_dims=[20], n_actions=7, mem_size=100000,
                       batch_size=64, eps_min=eps_min, eps_dec=eps_dec_per_game, replace=100,
                       checkpoint_dir=save_dir)

    best_avg_score = -np.inf
    if load_checkpoint:
        try:
            main_agent.load_models()
            print(">>> ê¸°ì¡´ buckshot_eval ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
            try:
                with open(_best_score_path(save_dir), "r") as f:
                    best_avg_score = float(f.read().strip())
                print(f"   ì´ì „ ê³ ì  ë¶ˆëŸ¬ì˜´: {best_avg_score:.1f}")
            except (FileNotFoundError, ValueError):
                pass
        except:
            print(">>> ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")

    env = GameEnvironment(max_hp=max_hp)
    
    scores_history = []
    eps_history = []

    print(f"ğŸš€ ìˆœìˆ˜ Self-Play í•™ìŠµ ì‹œì‘ (ì´ {num_games} ê²Œì„)")
    print(f"   ì²´í¬í¬ì¸íŠ¸: ê³ ì (êµ¬ê°„ í‰ê·  {checkpoint_interval}ê²Œì„) ê°±ì‹  ì‹œì—ë§Œ ì €ì¥")

    for game_num in range(1, num_games + 1):
        obs = env.reset() #
        done = False
        score = 0
        
        while not done:
            # í˜„ì¬ í„´ì¸ í”Œë ˆì´ì–´ì˜ ê´€ì ìœ¼ë¡œ ì‹œì  ë³€í™˜
            view = env.preprocess_state(obs)
            mask = env.get_action_mask()
            # ë©”ì¸ ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ì§€ëŠ¥ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ (BLUE/RED ê³µí†µ, action masking ì ìš©)
            action, _ = main_agent.choose_action(view, action_mask=mask)
            
            if env.current_turn == Player.BLUE:
                # BLUE í„´: í•™ìŠµ (store + learn + score)
                next_obs, reward, done, _ = env.step(action)
                # BLUEê°€ ì·¨í•œ ì•¡ì…˜ì˜ ê²°ê³¼ S'ëŠ” BLUE ê´€ì ìœ¼ë¡œ ìœ ì§€. preprocess_state(next_obs)ë¥¼ ì“°ë©´
                # í„´ì´ REDë¡œ ë°”ë€ ë’¤ RED ê´€ì ìœ¼ë¡œ ë’¤ì§‘í˜€ "ë‚´ HPâ†”ìƒëŒ€ HP" í™˜ê°ì„ ë°°ìš°ê²Œ ë¨.
                next_view = np.copy(next_obs)
                main_agent.store_transition(view, action, reward, next_view, int(done))
                main_agent.learn()
                score += reward
                obs = next_obs
            else:
                # RED í„´: í•™ìŠµ ì—†ì´ í™˜ê²½ë§Œ ì§„í–‰ (ì—ì´ì „íŠ¸ëŠ” BLUEë§Œ í•™ìŠµ)
                next_obs, _, done, _ = env.step(action)
                obs = next_obs

        scores_history.append(score)
        eps_history.append(main_agent.epsilon)
        main_agent.decrease_epsilon()  # ê²Œì„ 1íšŒë§ˆë‹¤ epsilon 1íšŒ ê°ì†Œ

        # ê³ ì (êµ¬ê°„ í‰ê· ) ê°±ì‹  ì‹œì—ë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if game_num >= checkpoint_interval:
            avg_score = np.mean(scores_history[-checkpoint_interval:])
            if avg_score > best_avg_score:
                diff = avg_score - best_avg_score
                best_avg_score = avg_score
                main_agent.save_models()
                try:
                    with open(_best_score_path(save_dir), "w") as f:
                        f.write(f"{best_avg_score:.6f}\n")
                except Exception:
                    pass
                print(f"Ep {game_num} | ğŸ†• ê³ ì  ê°±ì‹  â†’ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ | Avg: {avg_score:.1f} (ì´ì „ ëŒ€ë¹„ +{diff:.1f}) | Eps: {main_agent.epsilon:.4f}")
            elif game_num % checkpoint_interval == 0:
                print(f"Ep {game_num} | Avg Score: {avg_score:.1f} (ìµœê³ : {best_avg_score:.1f}) | Eps: {main_agent.epsilon:.4f}")
        elif game_num % checkpoint_interval == 0:
            avg_score = np.mean(scores_history[-game_num:]) if scores_history else 0.0
            print(f"Ep {game_num} | Avg Score: {avg_score:.1f} | Eps: {main_agent.epsilon:.4f}")

    # --- í•™ìŠµ ì¢…ë£Œ í›„ ê·¸ë˜í”„ ìƒì„± ---
    print(">>> í•™ìŠµ ì¢…ë£Œ. ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    x = [i+1 for i in range(len(scores_history))]
    graph_filename = 'pure_self_play_results.png'
    plotLearning(x, scores_history, eps_history, graph_filename)
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {graph_filename}")

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--games", type=int, default=10000, help="í•™ìŠµí•  ê²Œì„ ìˆ˜ (ì§§ê²Œ: 200~500)")
    p.add_argument("--no-load", action="store_true", help="ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„°")
    p.add_argument("--save-dir", type=str, default="Agents", help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ")
    args = p.parse_args()
    train_pure_self_play(
        num_games=args.games,
        load_checkpoint=not args.no_load,
        save_dir=args.save_dir,
    )