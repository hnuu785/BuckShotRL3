import numpy as np
import os
import sys
import random
import torch as T
import pandas as pd

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import Agent
from utils import plotLearning
from game_env import GameEnvironment, Player

def train_pure_self_play(
    num_games: int = 10000,
    checkpoint_interval: int = 300,
    save_dir: str = 'Agents',
    load_checkpoint: bool = True,
    max_hp: int = 4
):
    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    
    # ë©”ì¸ í•™ìŠµ ì—ì´ì „íŠ¸ í•˜ë‚˜ë§Œ ì‚¬ìš©
    main_agent = Agent(gamma=0.99, epsilon=0.1, lr=5e-5, 
                       input_dims=[20], n_actions=7, mem_size=100000, 
                       batch_size=64, eps_min=0.01, eps_dec=1e-6, replace=100, 
                       checkpoint_dir=save_dir)

    if load_checkpoint:
        try:
            main_agent.load_models()
            print(">>> ê¸°ì¡´ buckshot_eval ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except:
            print(">>> ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")

    env = GameEnvironment(max_hp=max_hp)
    
    scores_history = []
    eps_history = []
    best_avg_score = float('-inf')  # ìµœê³  í‰ê·  ì ìˆ˜ ì¶”ì 

    print(f"ğŸš€ ìˆœìˆ˜ Self-Play í•™ìŠµ ì‹œì‘ (ì´ {num_games} ê²Œì„)")

    for game_num in range(1, num_games + 1):
        obs = env.reset() #
        done = False
        score = 0
        
        while not done:
            # í˜„ì¬ í„´ì¸ í”Œë ˆì´ì–´ì˜ ê´€ì ìœ¼ë¡œ ì‹œì  ë³€í™˜
            view = env.preprocess_state(obs)
            
            # ë©”ì¸ ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ì§€ëŠ¥ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ (BLUE/RED ê³µí†µ)
            action, _ = main_agent.choose_action(view)
            
            if env.current_turn == Player.BLUE:
                # BLUE(0ë²ˆ) í„´: ì‹¤ì œ í•™ìŠµìš© ë°ì´í„°ë¥¼ ìŒ“ìŒ
                next_obs, reward, done, _ = env.step(action)
                
                # ë‹¤ìŒ ìƒíƒœë„ BLUE ê´€ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                next_view = env.preprocess_state(next_obs)
                main_agent.store_transition(view, action, reward, next_view, int(done))
                main_agent.learn()
                
                score += reward
                obs = next_obs
            else:
                # RED(1ë²ˆ) í„´: ì•¡ì…˜ë§Œ ìˆ˜í–‰ (í•™ìŠµ ë°ì´í„°ëŠ” ìŒ“ì§€ ì•ŠìŒ)
                # ì´ë¯¸ ìœ„ì—ì„œ main_agentì˜ actionì„ ë½‘ì•˜ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‹¤í–‰
                obs, _, done, _ = env.step(action)

        scores_history.append(score)
        eps_history.append(main_agent.epsilon)

        # ì£¼ê¸°ì ìœ¼ë¡œ ì„±ëŠ¥ ì²´í¬ ë° ê³ ì ì¼ ë•Œë§Œ ì €ì¥
        if game_num % checkpoint_interval == 0:
            avg_score = np.mean(scores_history[-checkpoint_interval:])
            
            if avg_score > best_avg_score:
                best_avg_score = avg_score
                main_agent.save_models()
                print(f"Ep {game_num} | Avg Score: {avg_score:.1f} | Eps: {main_agent.epsilon:.4f} | ğŸ† NEW BEST! ëª¨ë¸ ì €ì¥")
            else:
                print(f"Ep {game_num} | Avg Score: {avg_score:.1f} | Eps: {main_agent.epsilon:.4f} | Best: {best_avg_score:.1f}")

    # --- í•™ìŠµ ì¢…ë£Œ í›„ ê·¸ë˜í”„ ìƒì„± ---
    print(">>> í•™ìŠµ ì¢…ë£Œ. ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    x = [i+1 for i in range(len(scores_history))]
    graph_filename = 'pure_self_play_results.png'
    plotLearning(x, scores_history, eps_history, graph_filename)
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {graph_filename}")
    
    # --- ì—‘ì…€ íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥ ---
    print(">>> ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘...")
    df = pd.DataFrame({
        'Episode': x,
        'Score': scores_history,
        'Epsilon': eps_history
    })
    
    # ì´ë™ í‰ê·  ì¶”ê°€ (100 ì—í”¼ì†Œë“œ ê¸°ì¤€)
    df['Avg_Score_100'] = df['Score'].rolling(window=100, min_periods=1).mean()
    
    excel_filename = 'pure_self_play_results.xlsx'
    df.to_excel(excel_filename, index=False, sheet_name='Training Results')
    print(f"âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {excel_filename}")

if __name__ == "__main__":
    train_pure_self_play()