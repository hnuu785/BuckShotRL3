import numpy as np
import os
import sys
import random
import torch as T

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import Agent
from utils import plotLearning
from game_env import GameEnvironment, Player

def train_pure_self_play(
    num_games: int = 10000,
    checkpoint_interval: int = 300,
    save_dir: str = 'Agents',
    load_checkpoint: bool = True,
    max_hp: int = 4
):
    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    
    # ë©”ì¸ í•™ìŠµ ì—ì´ì „íŠ¸ í•˜ë‚˜ë§Œ ì‚¬ìš©
    main_agent = Agent(gamma=0.99, epsilon=0.1, lr=5e-5, 
                       input_dims=[20], n_actions=7, mem_size=100000, 
                       batch_size=64, eps_min=0.01, eps_dec=1e-6, replace=100, 
                       checkpoint_dir=save_dir)

    if load_checkpoint:
        try:
            main_agent.load_models()
            print(">>> ê¸°ì¡´ buckshot_eval ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except:
            print(">>> ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")

    env = GameEnvironment(max_hp=max_hp)
    
    scores_history = []
    eps_history = []

    print(f"ğŸš€ ìˆœìˆ˜ Self-Play í•™ìŠµ ì‹œì‘ (ì´ {num_games} ê²Œì„)")

    for game_num in range(1, num_games + 1):
        obs = env.reset() #
        done = False
        score = 0
        
        while not done:
            # í˜„ì¬ í„´ì¸ í”Œë ˆì´ì–´ì˜ ê´€ì ìœ¼ë¡œ ì‹œì  ë³€í™˜
            view = env.preprocess_state(obs)
            mask = env.get_action_mask()
            # ë©”ì¸ ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ì§€ëŠ¥ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ (BLUE/RED ê³µí†µ, action masking ì ìš©)
            action, _ = main_agent.choose_action(view, action_mask=mask)
            
            if env.current_turn == Player.BLUE:
                # BLUE(0ë²ˆ) í„´: í•™ìŠµìš© ë°ì´í„° ì €ì¥ (BLUE ê´€ì  ë³´ìƒ ê·¸ëŒ€ë¡œ)
                next_obs, reward, done, _ = env.step(action)
                next_view = env.preprocess_state(next_obs)
                main_agent.store_transition(view, action, reward, next_view, int(done))
                main_agent.learn()
                score += reward
                obs = next_obs
            else:
                # RED(1ë²ˆ) í„´: RED ê²½í—˜ë„ BLUE ê´€ì ìœ¼ë¡œ ì €ì¥ í›„ í•™ìŠµ
                # REDê°€ ì–»ì€ ë³´ìƒ = BLUE ì…ì¥ì—ì„  ì†í•´ì´ë¯€ë¡œ -reward ë¡œ ì €ì¥
                next_obs, reward, done, _ = env.step(action)
                next_view = env.preprocess_state(next_obs)
                main_agent.store_transition(view, action, -reward, next_view, int(done))
                main_agent.learn()
                obs = next_obs

        scores_history.append(score)
        eps_history.append(main_agent.epsilon)

        # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¶œë ¥ ë° ì €ì¥
        if game_num % 10 == 0:
            main_agent.save_models()
            
        if game_num % checkpoint_interval == 0:
            avg_score = np.mean(scores_history[-checkpoint_interval:])
            print(f"Ep {game_num} | Avg Score: {avg_score:.1f} | Eps: {main_agent.epsilon:.4f}")

    # --- í•™ìŠµ ì¢…ë£Œ í›„ ê·¸ë˜í”„ ìƒì„± ---
    print(">>> í•™ìŠµ ì¢…ë£Œ. ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    x = [i+1 for i in range(len(scores_history))]
    graph_filename = 'pure_self_play_results.png'
    plotLearning(x, scores_history, eps_history, graph_filename)
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {graph_filename}")

if __name__ == "__main__":
    train_pure_self_play()